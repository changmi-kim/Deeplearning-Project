{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 손 동작 인식\n",
    "- 01. LSTM\n",
    "- 02. KNN\n",
    "- 03. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 15:55:53.391875: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-14 15:55:53.391909: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-14 15:55:53.392865: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-14 15:55:53.398118: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-14 15:55:54.356371: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers, activations, initializers, losses, optimizers, metrics\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 확인\n",
    "# npy_data = np.load('./dataset/seq_go_1700819427.npy')\n",
    "# npy_data = np.load('./dataset/seq_back_1700819496.npy')\n",
    "# npy_data = np.load('./dataset/seq_go_1700819427.npy')\n",
    "# npy_data = np.load('./dataset/seq_go_1700819427.npy')\n",
    "# npy_data = np.load('./dataset/seq_go_1700819427.npy')\n",
    "\n",
    "# print(npy_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npy_data = np.load('../data/dataset_opensource_bbang/raw_away_1627646273.npy')\n",
    "npy_data = np.load(\"../data/dataset_opensource_bbang/seq_away_1627646273.npy\")\n",
    "\n",
    "print(npy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-1. LSTM - 데이터셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_hands = 1\n",
    "actions = ['back']\n",
    "# actions = ['go', 'back', 'stop', 'left_spin', 'right_spin', 'speed_up', 'speed_down', 'bad_gesture']\n",
    "\n",
    "# 시퀀스 길이 지정\n",
    "seq_length = 40\n",
    "secs_for_action = 2\n",
    "\n",
    "# 미디어파이프 패키지에서 손 인식을 위한 객체 생성\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands = max_num_hands,\n",
    "    min_detection_confidence = 0.5,\n",
    "    min_tracking_confidence = 0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "created_time = str(time.strftime('%X', time.localtime(time.time())))\n",
    "# exist_ok를 True로 설정하지 않았을 땐, 해당 디렉토리가 존재하는 경우 exception 에러 발생\n",
    "os.makedirs('../data/dataset', exist_ok=True)\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    for idx, action in enumerate(actions):\n",
    "\n",
    "        data = []\n",
    "\n",
    "        ret, img = cap.read()\n",
    "        if not ret:\n",
    "            print(\"카메라 연결 실패\")\n",
    "            # break\n",
    "            continue\n",
    "\n",
    "        img = cv2.flip(img, 1)\n",
    "        cv2.putText(img, f'Waiting for collecting {action.upper()} action...', org=(10, 30), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "        cv2.imshow('Dataset', img)\n",
    "        cv2.waitKey(3000)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        while time.time() - start_time < secs_for_action:\n",
    "\n",
    "            ret, img = cap.read()\n",
    "            img = cv2.flip(img, 1)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            result = hands.process(img)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if result.multi_hand_landmarks is not None:\n",
    "\n",
    "                for res in result.multi_hand_landmarks:\n",
    "\n",
    "                    joint = np.zeros((21, 4))\n",
    "\n",
    "                    for j, lm in enumerate(res.landmark):\n",
    "                        \n",
    "                        joint[j] = [lm.x, lm.y, lm.z, lm.visibility]\n",
    "\n",
    "                    v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3]\n",
    "                    v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3]\n",
    "                    v = v2 - v1\n",
    "\n",
    "                    v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "                    angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                        v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "                        v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:]))\n",
    "\n",
    "                    angle = np.degrees(angle)\n",
    "\n",
    "                    angle_label = np.array([angle], dtype=np.float32)\n",
    "                    angle_label = np.append(angle_label, 0)\n",
    "\n",
    "                    d = np.concatenate([joint.flatten(), angle_label])\n",
    "\n",
    "                    data.append(d)\n",
    "                    # print(data)\n",
    "\n",
    "                    mp_drawing.draw_landmarks(img,\n",
    "                            res,\n",
    "                            mp_hands.HAND_CONNECTIONS\n",
    "                        #   mp_hands.get_default_hand_landmarks_style(),\n",
    "                        #   mp_hands.get_default_hand_connections_style()\n",
    "                    )\n",
    "\n",
    "            cv2.imshow('Dataset', img)\n",
    "            \n",
    "            key = cv2.waitKey(5) & 0xFF\n",
    "            \n",
    "            if key == 27:  # ESC를 눌렀을 경우\n",
    "                cv2.destroyAllWindows()\n",
    "                cap.release()  # 비디오 캡처 객체 해제\n",
    "                break\n",
    "\n",
    "        data = np.array(data)\n",
    "        print(action, data.shape)\n",
    "        np.save(os.path.join('../data', f'raw_{action}_{created_time}'), data)\n",
    "\n",
    "        # seq 데이터 저장\n",
    "        full_seq_data = []\n",
    "        for seq in range(len(data) - seq_length):\n",
    "            full_seq_data.append(data[seq:seq + seq_length])\n",
    "\n",
    "        full_seq_data = np.array(full_seq_data)\n",
    "        print(action, full_seq_data.shape)\n",
    "        np.save(os.path.join('../data', f'seq_{action}_{created_time}'), full_seq_data)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-2. LSTM - 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터셋_10\n",
    "# data_go = np.loadtxt('../data/dataset_10_231211/go_04:54:34 PM.csv', delimiter=',')\n",
    "# data_back= np.loadtxt('../data/dataset_10_231211/back_04:55:04 PM.csv', delimiter=',')\n",
    "# data_stop= np.loadtxt('../data/dataset_10_231211/stop_04:55:31 PM.csv', delimiter=',')\n",
    "# data_left= np.loadtxt('../data/dataset_10_231211/left_spin_04:55:49 PM.csv', delimiter=',')\n",
    "# data_right= np.loadtxt('../data/dataset_10_231211/right_spin_04:56:08 PM.csv', delimiter=',')\n",
    "# data_up= np.loadtxt('../data/dataset_10_231211/speed_up_04:57:54 PM.csv', delimiter=',')\n",
    "# data_down= np.loadtxt('../data/dataset_10_231211/speed_down_04:58:18 PM.csv', delimiter=',')\n",
    "# data_bad= np.loadtxt('../data/dataset_10_231211/bad_gesture_05:00:03 PM.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_length = 4\n",
    "# full_seq_data = []\n",
    "\n",
    "# for seq in range(len(data) - seq_length):\n",
    "#     full_seq_data.append(data[seq:seq + seq_length])\n",
    "\n",
    "# full_seq_data = np.array(full_seq_data)\n",
    "# np.save('../data/dataset_10_231211/go_04:54:34 PM.npy', full_seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/dataset_100_231211/\"\n",
    "file_list = os.listdir(path)\n",
    "file_list_csv = [file for file in file_list if file.endswith(\".csv\")]\n",
    "\n",
    "for idx, item in enumerate(file_list_csv):\n",
    "    if 'total' in item:\n",
    "        file_list_csv.pop(idx)\n",
    "        \n",
    "print (\"file_list_csv: {}\".format(file_list_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 4\n",
    "path = \"../data/dataset_100_231211/\"\n",
    "\n",
    "for i in file_list_csv:\n",
    "\n",
    "    data = np.loadtxt(os.path.join(path, i), delimiter=',')\n",
    "    \n",
    "    full_seq_data = []\n",
    "\n",
    "    for seq in range(len(data) - seq_length):\n",
    "        full_seq_data.append(data[seq:seq + seq_length])\n",
    "\n",
    "    full_seq_data = np.array(full_seq_data)\n",
    "    file_name = i[:-3] + 'npy'\n",
    "    \n",
    "    np.save(os.path.join(path, file_name), full_seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_data_create = np.load('../data/dataset_100_231211/go_05:04:05 PM.npy')\n",
    "print(npy_data_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(npy_data_create.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['go', 'back', 'stop', 'left_spin', 'right_spin', 'speed_up', 'speed_down', 'bad_gesture']\n",
    "\n",
    "# 데이터셋_10\n",
    "data = np.concatenate([\n",
    "    np.load('../data/dataset_10_231211/go_04:54:34 PM.npy'),\n",
    "    np.load('../data/dataset_10_231211/back_04:55:04 PM.npy'),\n",
    "    np.load('../data/dataset_10_231211/stop_04:55:31 PM.npy'),\n",
    "    np.load('../data/dataset_10_231211/left_spin_04:55:49 PM.npy'),\n",
    "    np.load('../data/dataset_10_231211/right_spin_04:56:08 PM.npy'),\n",
    "    np.load('../data/dataset_10_231211/speed_up_04:57:54 PM.npy'),\n",
    "    np.load('../data/dataset_10_231211/speed_down_04:58:18 PM.npy'),\n",
    "    np.load('../data/dataset_10_231211/bad_gesture_05:00:03 PM.npy')\n",
    "], axis=0)\n",
    "\n",
    "# 데이터셋_100\n",
    "# data = np.concatenate([\n",
    "#     np.load('../data/dataset_100_231211/go_05:04:05 PM.npy'),\n",
    "#     np.load('../data/dataset_100_231211/back_05:06:55 PM.npy'),\n",
    "#     np.load('../data/dataset_100_231211/stop_05:09:41 PM.npy'),\n",
    "#     np.load('../data/dataset_100_231211/left_spin_05:12:50 PM.npy'),\n",
    "#     np.load('../data/dataset_100_231211/right_spin_05:15:07 PM.npy'),\n",
    "#     np.load('../data/dataset_100_231211/speed_up_05:18:02 PM.npy'),\n",
    "#     np.load('../data/dataset_100_231211/speed_down_05:20:26 PM.npy'),\n",
    "#     np.load('../data/dataset_100_231211/bad_gesture_05:23:23 PM.npy')\n",
    "# ], axis=0)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data[:, :, :-1]\n",
    "labels = data[:, 0, -1]\n",
    "\n",
    "print(x_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = to_categorical(labels, num_classes=None)\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data,\n",
    "                                                  y_data,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=2023,\n",
    "                                                  stratify=y_data)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=x_train.shape[1:3]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(actions), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=500,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint('../model/model_.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(x_val, y_val)\n",
    "\n",
    "print('loss (cross-entropy) :', result[0])\n",
    "print('test accuracy :', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 모델\n",
    "# 이진 분류\n",
    "fig, axes = plt.subplots(1, 2, figsize = (12, 4))\n",
    "\n",
    "sns.lineplot(x = range(len(history.history[\"loss\"])),\n",
    "             y = history.history[\"loss\"], ax = axes[0],\n",
    "             label = 'Training Loss')\n",
    "\n",
    "sns.lineplot(x = range(len(history.history[\"loss\"])),\n",
    "             y = history.history[\"val_loss\"], ax = axes[0],\n",
    "             label = 'Validation Loss')\n",
    "\n",
    "\n",
    "sns.lineplot(x = range(len(history.history[\"acc\"])),\n",
    "             y = history.history[\"acc\"], ax = axes[1],\n",
    "             label = 'Training Accuracy')\n",
    "\n",
    "sns.lineplot(x = range(len(history.history[\"acc\"])),\n",
    "             y = history.history[\"val_acc\"], ax = axes[1],\n",
    "             label = 'Validation Accuracy')\n",
    "axes[0].set_title(\"Loss\"); axes[1].set_title(\"Accuracy\")\n",
    "\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../model/model_.h5')\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "\n",
    "multilabel_confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-3. LSTM - 성능 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2,3,4,5]\n",
    "a[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 8가지 제스처 인식 Test\n",
    "# max_num_hands = 1\n",
    "# actions = ['go', 'back', 'stop', 'left_spin', 'right_spin', 'speed_up', 'speed_down', 'bad_gesture']\n",
    "# gestures = {\n",
    "#     0:'go', 1:'back', 2:'stop', 3:'left_spin', 4:'right_spin', 5:'speed_up',\n",
    "#     6:'speed_down', 7:'bad_gesture'}\n",
    "\n",
    "# # 시퀀스 길이 지정\n",
    "# seq_length = 4\n",
    "\n",
    "# # 미디어파이프 패키지에서 손 인식을 위한 객체 생성\n",
    "# mp_hands = mp.solutions.hands\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# hands = mp_hands.Hands(\n",
    "#     max_num_hands = max_num_hands,\n",
    "#     min_detection_confidence = 0.5,\n",
    "#     min_tracking_confidence = 0.5)\n",
    "\n",
    "# model = keras.models.load_model(\"../model/LSTM_model_dataset100_epoch500.h5\")\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# seq = []\n",
    "# action_seq = []\n",
    "\n",
    "# while cap.isOpened():\n",
    "\n",
    "#     ret, img = cap.read()\n",
    "#     if not ret:\n",
    "#         print(\"카메라 연결 실패\")\n",
    "#         # break\n",
    "#         continue\n",
    "    \n",
    "#     img = cv2.flip(img, 1)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     result = hands.process(img)\n",
    "\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#     # 손 인식 여부 확인\n",
    "#     if result.multi_hand_landmarks is not None:\n",
    "\n",
    "#         for res in result.multi_hand_landmarks:\n",
    "        \n",
    "#             joint = np.zeros((21, 4))\n",
    "        \n",
    "#             for j, lm in enumerate(res.landmark):\n",
    "#                 joint[j] = [lm.x, lm.y, lm.z, lm.visibility]\n",
    "\n",
    "#             v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3]\n",
    "#             v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3]\n",
    "#             v = v2 - v1 # [20, 3]\n",
    "\n",
    "#             v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "#             angle = np.arccos(np.einsum('nt,nt->n',\n",
    "#                 v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "#                 v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "\n",
    "#             angle = np.degrees(angle)\n",
    "\n",
    "#             data = angle\n",
    "\n",
    "#             # data = np.array([angle], dtype=np.float32)\n",
    "\n",
    "#             seq.append(data)\n",
    "\n",
    "#             mp_drawing.draw_landmarks(img,\n",
    "#                                       res,\n",
    "#                                       mp_hands.HAND_CONNECTIONS\n",
    "#                                     #   mp_hands.get_default_hand_landmarks_style(),\n",
    "#                                     #   mp_hands.get_default_hand_connections_style()\n",
    "#             )\n",
    "            \n",
    "#             if len(seq) < seq_length:\n",
    "#                 continue\n",
    "\n",
    "#             # print(seq[-seq_length:])\n",
    "\n",
    "#             input_data = np.expand_dims(np.array(seq[-seq_length:], dtype=np.float32), axis=0)\n",
    "\n",
    "#             # print(input_data)\n",
    "#             y_pred = model.predict(input_data).squeeze()\n",
    "\n",
    "#             i_pred = int(np.argmax(y_pred))\n",
    "#             conf = y_pred[i_pred]\n",
    "\n",
    "#             if conf < 0.9:\n",
    "#                 continue\n",
    "\n",
    "#             action = actions[i_pred]\n",
    "#             action_seq.append(action)\n",
    "\n",
    "#             if len(action_seq) < 8:\n",
    "#                 continue\n",
    "\n",
    "#             this_action = '?'\n",
    "#             if action_seq[-1] == action_seq[-2] == action_seq[-3]:\n",
    "#                 this_action = action\n",
    "\n",
    "#             # idx = encoder.inverse_transform(model.predict([data]))[0,0]\n",
    "\n",
    "#             # if idx in gestures.keys():\n",
    "#             #     cv2.putText(img, text=gestures[idx].upper(), org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "            \n",
    "#             cv2.putText(img, f'{this_action.upper()}', org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "\n",
    "#     cv2.imshow('Test', img)\n",
    "\n",
    "#     key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "#     if key == 27:\n",
    "#         cv2.destroyAllWindows()\n",
    "#         cap.release()\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8가지 제스처 인식 Test\n",
    "# 한손만 인식\n",
    "max_num_hands = 1\n",
    "actions = ['go', 'back', 'stop', 'left_spin', 'right_spin', 'speed_up', 'speed_down', 'bad_gesture']\n",
    "gestures = {\n",
    "    0:'go', 1:'back', 2:'stop', 3:'left_spin', 4:'right_spin', 5:'speed_up',\n",
    "    6:'speed_down', 7:'bad_gesture'}\n",
    "\n",
    "# 시퀀스 길이 지정\n",
    "seq_length = 4\n",
    "\n",
    "# 미디어파이프 패키지에서 손 인식을 위한 객체 생성\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands = max_num_hands,\n",
    "    min_detection_confidence = 0.5,\n",
    "    min_tracking_confidence = 0.5)\n",
    "\n",
    "model = keras.models.load_model(\"../model/LSTM_model_dataset10_epoch500.h5\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "seq = []\n",
    "action_seq = []\n",
    "\n",
    "fps = 0\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "prev_action = '?'\n",
    "current_action = '?'\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    ret, img = cap.read()\n",
    "    if not ret:\n",
    "        print(\"카메라 연결 실패\")\n",
    "        # break\n",
    "        continue\n",
    "\n",
    "    frame_count += 1\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time > 1.0:  # 1초마다 FPS 갱신\n",
    "        fps = frame_count / elapsed_time\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "    \n",
    "    img = cv2.flip(img, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result = hands.process(img)\n",
    "\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # 손 인식 여부 확인\n",
    "    if result.multi_hand_landmarks is not None:\n",
    "\n",
    "        for res in result.multi_hand_landmarks:\n",
    "        \n",
    "            joint = np.zeros((21, 4))\n",
    "        \n",
    "            for j, lm in enumerate(res.landmark):\n",
    "                joint[j] = [lm.x, lm.y, lm.z, lm.visibility]\n",
    "\n",
    "            v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3]\n",
    "            v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3]\n",
    "            v = v2 - v1 # [20, 3]\n",
    "\n",
    "            v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "            angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "                v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "\n",
    "            angle = np.degrees(angle)\n",
    "\n",
    "            data = angle\n",
    "\n",
    "            # data = np.array([angle], dtype=np.float32)\n",
    "\n",
    "            seq.append(data)\n",
    "\n",
    "            mp_drawing.draw_landmarks(img,\n",
    "                                      res,\n",
    "                                      mp_hands.HAND_CONNECTIONS\n",
    "                                    #   mp_hands.get_default_hand_landmarks_style(),\n",
    "                                    #   mp_hands.get_default_hand_connections_style()\n",
    "            )\n",
    "            \n",
    "            if len(seq) < seq_length:\n",
    "                continue\n",
    "\n",
    "            # print(seq[-seq_length:])\n",
    "\n",
    "            input_data = np.expand_dims(np.array(seq[-seq_length:], dtype=np.float32), axis=0)\n",
    "\n",
    "            # print(input_data)\n",
    "            y_pred = model.predict(input_data).squeeze()\n",
    "\n",
    "            i_pred = int(np.argmax(y_pred))\n",
    "            conf = y_pred[i_pred]\n",
    "\n",
    "            if conf < 0.9:\n",
    "                continue\n",
    "\n",
    "            action = actions[i_pred]\n",
    "            action_seq.append(action)\n",
    "\n",
    "            if len(action_seq) < 8:\n",
    "                continue\n",
    "\n",
    "            if len(action_seq) >= 8:\n",
    "                prev_action = action_seq[-8]\n",
    "                current_action = action_seq[-1]\n",
    "\n",
    "            this_action = '?'\n",
    "            if action_seq[-1] == action_seq[-2] == action_seq[-3]:\n",
    "                this_action = action\n",
    "\n",
    "            # idx = encoder.inverse_transform(model.predict([data]))[0,0]\n",
    "\n",
    "            # if idx in gestures.keys():\n",
    "            #     cv2.putText(img, text=gestures[idx].upper(), org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "            \n",
    "            cv2.putText(img, f'{this_action.upper()}', org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        cv2.putText(img, f'Prev Action: {prev_action.upper()}', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.putText(img, f'Current Action: {current_action.upper()}', (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Test', img)\n",
    "\n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "    if key == 27:\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8가지 제스처 인식 Test\n",
    "# 영상용\n",
    "# 한손만 인식\n",
    "max_num_hands = 1\n",
    "actions = ['go', 'back', 'stop', 'left_spin', 'right_spin', 'speed_up', 'speed_down', 'bad_gesture']\n",
    "gestures = {\n",
    "    0:'go', 1:'back', 2:'stop', 3:'left_spin', 4:'right_spin', 5:'speed_up',\n",
    "    6:'speed_down', 7:'bad_gesture'}\n",
    "\n",
    "# 시퀀스 길이 지정\n",
    "seq_length = 4\n",
    "\n",
    "# 미디어파이프 패키지에서 손 인식을 위한 객체 생성\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands = max_num_hands,\n",
    "    min_detection_confidence = 0.5,\n",
    "    min_tracking_confidence = 0.5)\n",
    "\n",
    "model = keras.models.load_model(\"../model/LSTM_model_dataset100_epoch500.h5\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "seq = []\n",
    "action_seq = []\n",
    "\n",
    "fps = 0\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# prev_action = '?'\n",
    "# current_action = '?'\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    ret, img = cap.read()\n",
    "    if not ret:\n",
    "        print(\"카메라 연결 실패\")\n",
    "        # break\n",
    "        continue\n",
    "\n",
    "    frame_count += 1\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time > 1.0:  # 1초마다 FPS 갱신\n",
    "        fps = frame_count / elapsed_time\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "    \n",
    "    img = cv2.flip(img, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result = hands.process(img)\n",
    "\n",
    "    cv2.putText(img, 'LSTM', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # 손 인식 여부 확인\n",
    "    if result.multi_hand_landmarks is not None:\n",
    "\n",
    "        for res in result.multi_hand_landmarks:\n",
    "        \n",
    "            joint = np.zeros((21, 4))\n",
    "        \n",
    "            for j, lm in enumerate(res.landmark):\n",
    "                joint[j] = [lm.x, lm.y, lm.z, lm.visibility]\n",
    "\n",
    "            v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3]\n",
    "            v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3]\n",
    "            v = v2 - v1 # [20, 3]\n",
    "\n",
    "            v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "            angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "                v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "\n",
    "            angle = np.degrees(angle)\n",
    "\n",
    "            data = angle\n",
    "\n",
    "            # data = np.array([angle], dtype=np.float32)\n",
    "\n",
    "            seq.append(data)\n",
    "\n",
    "            mp_drawing.draw_landmarks(img,\n",
    "                                      res,\n",
    "                                      mp_hands.HAND_CONNECTIONS\n",
    "                                    #   mp_hands.get_default_hand_landmarks_style(),\n",
    "                                    #   mp_hands.get_default_hand_connections_style()\n",
    "            )\n",
    "            \n",
    "            if len(seq) < seq_length:\n",
    "                continue\n",
    "\n",
    "            # print(seq[-seq_length:])\n",
    "\n",
    "            input_data = np.expand_dims(np.array(seq[-seq_length:], dtype=np.float32), axis=0)\n",
    "\n",
    "            # print(input_data)\n",
    "            y_pred = model.predict(input_data).squeeze()\n",
    "\n",
    "            i_pred = int(np.argmax(y_pred))\n",
    "            conf = y_pred[i_pred]\n",
    "\n",
    "            if conf < 0.9:\n",
    "                continue\n",
    "\n",
    "            action = actions[i_pred]\n",
    "            action_seq.append(action)\n",
    "\n",
    "            if len(action_seq) < 8:\n",
    "                continue\n",
    "\n",
    "            if len(action_seq) >= 8:\n",
    "                prev_action = action_seq[-8]\n",
    "                current_action = action_seq[-1]\n",
    "\n",
    "            this_action = '?'\n",
    "            if action_seq[-1] == action_seq[-2] == action_seq[-3]:\n",
    "                this_action = action\n",
    "                this_num = i_pred\n",
    "            # idx = encoder.inverse_transform(model.predict([data]))[0,0]\n",
    "\n",
    "            # if idx in gestures.keys():\n",
    "            #     cv2.putText(img, text=gestures[idx].upper(), org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "            \n",
    "            cv2.putText(img, f'{this_action.upper()}', org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=2, color=(0, 255, 0), thickness=7)\n",
    "            cv2.putText(img, f'Action: {this_num}. {prev_action.upper()}', (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        \n",
    "        # cv2.putText(img, f'Current Action: {current_action.upper()}', (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Test', img)\n",
    "\n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "    if key == 27:\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 8가지 제스처 인식 Test\n",
    "# # 양손 인식\n",
    "# max_num_hands = 1\n",
    "# actions = ['go', 'back', 'stop', 'left_spin', 'right_spin', 'speed_up', 'speed_down', 'bad_gesture']\n",
    "# gestures = {\n",
    "#     0:'go', 1:'back', 2:'stop', 3:'left_spin', 4:'right_spin', 5:'speed_up',\n",
    "#     6:'speed_down', 7:'bad_gesture'}\n",
    "\n",
    "# # 시퀀스 길이 지정\n",
    "# seq_length = 4\n",
    "\n",
    "# # 미디어파이프 패키지에서 손 인식을 위한 객체 생성\n",
    "# mp_hands = mp.solutions.hands\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# hands = mp_hands.Hands(\n",
    "#     max_num_hands = max_num_hands,\n",
    "#     min_detection_confidence = 0.5,\n",
    "#     min_tracking_confidence = 0.5)\n",
    "\n",
    "# model = keras.models.load_model(\"../model/LSTM_model_dataset100_epoch500.h5\")\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# left_seq = []\n",
    "# left_action_seq = []\n",
    "# right_seq = []\n",
    "# right_action_seq = []\n",
    "\n",
    "# fps = 0\n",
    "# frame_count = 0\n",
    "# start_time = time.time()\n",
    "\n",
    "# prev_action = '?'\n",
    "# current_action = '?'\n",
    "\n",
    "# while cap.isOpened():\n",
    "\n",
    "#     ret, img = cap.read()\n",
    "#     if not ret:\n",
    "#         print(\"카메라 연결 실패\")\n",
    "#         # break\n",
    "#         continue\n",
    "\n",
    "#     frame_count += 1\n",
    "#     elapsed_time = time.time() - start_time\n",
    "#     if elapsed_time > 1.0:  # 1초마다 FPS 갱신\n",
    "#         fps = frame_count / elapsed_time\n",
    "#         frame_count = 0\n",
    "#         start_time = time.time()\n",
    "    \n",
    "#     img = cv2.flip(img, 1)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     result = hands.process(img)\n",
    "\n",
    "#     cv2.putText(img, f'FPS: {int(fps)}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#     # 손 인식 여부 확인\n",
    "#     if result.multi_hand_landmarks is not None:\n",
    "\n",
    "#         for res in result.multi_hand_landmarks:\n",
    "        \n",
    "#             joint = np.zeros((21, 4))\n",
    "        \n",
    "#             for j, lm in enumerate(res.landmark):\n",
    "#                 joint[j] = [lm.x, lm.y, lm.z, lm.visibility]\n",
    "\n",
    "#             v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3]\n",
    "#             v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3]\n",
    "#             v = v2 - v1 # [20, 3]\n",
    "\n",
    "#             v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "#             angle = np.arccos(np.einsum('nt,nt->n',\n",
    "#                 v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "#                 v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "\n",
    "#             angle = np.degrees(angle)\n",
    "\n",
    "#             data = angle\n",
    "\n",
    "#             # data = np.array([angle], dtype=np.float32)\n",
    "\n",
    "#             seq.append(data)\n",
    "\n",
    "#             mp_drawing.draw_landmarks(img,\n",
    "#                                       res,\n",
    "#                                       mp_hands.HAND_CONNECTIONS\n",
    "#                                     #   mp_hands.get_default_hand_landmarks_style(),\n",
    "#                                     #   mp_hands.get_default_hand_connections_style()\n",
    "#             )\n",
    "            \n",
    "#             if len(seq) < seq_length:\n",
    "#                 continue\n",
    "\n",
    "#             # print(seq[-seq_length:])\n",
    "\n",
    "#             input_data = np.expand_dims(np.array(seq[-seq_length:], dtype=np.float32), axis=0)\n",
    "\n",
    "#             # print(input_data)\n",
    "#             y_pred = model.predict(input_data).squeeze()\n",
    "\n",
    "#             i_pred = int(np.argmax(y_pred))\n",
    "#             conf = y_pred[i_pred]\n",
    "\n",
    "#             if conf < 0.9:\n",
    "#                 continue\n",
    "\n",
    "#             action = actions[i_pred]\n",
    "#             action_seq.append(action)\n",
    "\n",
    "#             if len(action_seq) < 8:\n",
    "#                 continue\n",
    "\n",
    "#             if len(action_seq) >= 8:\n",
    "#                 prev_action = action_seq[-8]\n",
    "#                 current_action = action_seq[-1]\n",
    "\n",
    "#             this_action = '?'\n",
    "#             if action_seq[-1] == action_seq[-2] == action_seq[-3]:\n",
    "#                 this_action = action\n",
    "\n",
    "#             # idx = encoder.inverse_transform(model.predict([data]))[0,0]\n",
    "\n",
    "#             # if idx in gestures.keys():\n",
    "#             #     cv2.putText(img, text=gestures[idx].upper(), org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "            \n",
    "#             cv2.putText(img, f'{this_action.upper()}', org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "#         cv2.putText(img, f'Prev Action: {prev_action.upper()}', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "#         cv2.putText(img, f'Current Action: {current_action.upper()}', (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "#     cv2.imshow('Test', img)\n",
    "\n",
    "#     key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "#     if key == 27:\n",
    "#         cv2.destroyAllWindows()\n",
    "#         cap.release()\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-4. 최종 코드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers, activations, initializers, losses, optimizers, metrics\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pyautogui\n",
    "import time\n",
    "\n",
    "# 8가지 제스처 인식 Test\n",
    "# 영상용\n",
    "# 한손만 인식\n",
    "max_num_hands = 1\n",
    "actions = ['go', 'back', 'stop', 'left_spin', 'right_spin', 'speed_up', 'speed_down', 'bad_gesture']\n",
    "gestures = {\n",
    "    0:'go', 1:'back', 2:'stop', 3:'left_spin', 4:'right_spin', 5:'speed_up',\n",
    "    6:'speed_down', 7:'bad_gesture'}\n",
    "\n",
    "# 시퀀스 길이 지정\n",
    "seq_length = 4\n",
    "\n",
    "# 미디어파이프 패키지에서 손 인식을 위한 객체 생성\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands = max_num_hands,\n",
    "    min_detection_confidence = 0.5,\n",
    "    min_tracking_confidence = 0.5)\n",
    "\n",
    "model = keras.models.load_model(\"../model/LSTM_model_dataset100_epoch500.h5\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "seq = []\n",
    "action_seq = []\n",
    "\n",
    "fps = 0\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "prev_action = '?'\n",
    "current_action = '?'\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    ret, img = cap.read()\n",
    "    if not ret:\n",
    "        print(\"카메라 연결 실패\")\n",
    "        # break\n",
    "        continue\n",
    "\n",
    "    frame_count += 1\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time > 1.0:  # 1초마다 FPS 갱신\n",
    "        fps = frame_count / elapsed_time\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "    \n",
    "    img = cv2.flip(img, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result = hands.process(img)\n",
    "\n",
    "    cv2.putText(img, 'Hand Gesture Detecting System', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # 손 인식 여부 확인\n",
    "    if result.multi_hand_landmarks is not None:\n",
    "\n",
    "        for res in result.multi_hand_landmarks:\n",
    "        \n",
    "            joint = np.zeros((21, 4))\n",
    "        \n",
    "            for j, lm in enumerate(res.landmark):\n",
    "                joint[j] = [lm.x, lm.y, lm.z, lm.visibility]\n",
    "\n",
    "            v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3]\n",
    "            v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3]\n",
    "            v = v2 - v1 # [20, 3]\n",
    "\n",
    "            v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "            angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "                v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "\n",
    "            angle = np.degrees(angle)\n",
    "\n",
    "            data = angle\n",
    "\n",
    "            # data = np.array([angle], dtype=np.float32)\n",
    "\n",
    "            seq.append(data)\n",
    "\n",
    "            mp_drawing.draw_landmarks(img,\n",
    "                                      res,\n",
    "                                      mp_hands.HAND_CONNECTIONS\n",
    "                                    #   mp_hands.get_default_hand_landmarks_style(),\n",
    "                                    #   mp_hands.get_default_hand_connections_style()\n",
    "            )\n",
    "            \n",
    "            if len(seq) < seq_length:\n",
    "                continue\n",
    "\n",
    "            # print(seq[-seq_length:])\n",
    "\n",
    "            input_data = np.expand_dims(np.array(seq[-seq_length:], dtype=np.float32), axis=0)\n",
    "\n",
    "            # print(input_data)\n",
    "            y_pred = model.predict(input_data).squeeze()\n",
    "\n",
    "            i_pred = int(np.argmax(y_pred))\n",
    "            conf = y_pred[i_pred]\n",
    "\n",
    "            if conf < 0.9:\n",
    "                continue\n",
    "\n",
    "            action = actions[i_pred]\n",
    "            action_seq.append(action)\n",
    "\n",
    "            if len(action_seq) < 8:\n",
    "                continue\n",
    "\n",
    "            if len(action_seq) >= 8:\n",
    "                prev_action = action_seq[-8]\n",
    "                current_action = action_seq[-1]\n",
    "\n",
    "            this_action = '?'\n",
    "            if action_seq[-1] == action_seq[-2] == action_seq[-3]:\n",
    "                this_action = action\n",
    "                this_num = i_pred\n",
    "            # idx = encoder.inverse_transform(model.predict([data]))[0,0]\n",
    "\n",
    "            # if idx in gestures.keys():\n",
    "            #     cv2.putText(img, text=gestures[idx].upper(), org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "            \n",
    "            cv2.putText(img, f'{this_action.upper()}', org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=2, color=(0, 255, 0), thickness=7)\n",
    "        \n",
    "        cv2.putText(img, f'Prev Action: {prev_action.upper()}', (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.putText(img, f'Current Action: {current_action.upper()}', (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Test', img)\n",
    "\n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "    if key == 27:\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size(width=3000, height=1920)\n"
     ]
    }
   ],
   "source": [
    "import pyautogui\n",
    "print(pyautogui.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point(x=551, y=1339)\n"
     ]
    }
   ],
   "source": [
    "time.sleep(2)\n",
    "print(pyautogui.position())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pyautogui\n",
    "import time\n",
    "\n",
    "THRESHOLD = 0.2 # 20%, 값이 클수록 손이 카메라와 가까워야 인식함\n",
    "action_done = False\n",
    "\n",
    "# 8가지 제스처 인식 Testv\n",
    "# 영상용\n",
    "# 한손만 인식\n",
    "max_num_hands = 1\n",
    "actions = ['go', 'back', 'stop', 'left_spin', 'right_spin', 'speed_up', 'speed_down', 'bad_gesture']\n",
    "gestures = {\n",
    "    0:'go', 1:'back', 2:'stop', 3:'left_spin', 4:'right_spin', 5:'speed_up',\n",
    "    6:'speed_down', 7:'bad_gesture'}\n",
    "\n",
    "# 시퀀스 길이 지정\n",
    "seq_length = 4\n",
    "\n",
    "# 미디어파이프 패키지에서 손 인식을 위한 객체 생성\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands = max_num_hands,\n",
    "    min_detection_confidence = 0.5,\n",
    "    min_tracking_confidence = 0.5)\n",
    "\n",
    "model = keras.models.load_model(\"../model/LSTM_model_dataset100_epoch500.h5\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "seq = []\n",
    "action_seq = []\n",
    "\n",
    "fps = 0\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "key_input_mode = 'OFF'\n",
    "prev_action = '?'\n",
    "current_action = '?'\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    ret, img = cap.read()\n",
    "    if not ret:\n",
    "        print(\"카메라 연결 실패\")\n",
    "        # break\n",
    "        continue\n",
    "\n",
    "    frame_count += 1\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time > 1.0:  # 1초마다 FPS 갱신\n",
    "        fps = frame_count / elapsed_time\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "    \n",
    "    img = cv2.flip(img, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result = hands.process(img)\n",
    "\n",
    "    cv2.putText(img, 'Hand Gesture Detecting System', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.putText(img, f'Key Auto Input Mode: {key_input_mode}', (10, 470), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # 손 인식 여부 확인\n",
    "    if result.multi_hand_landmarks is not None:\n",
    "\n",
    "        for res in result.multi_hand_landmarks:\n",
    "        \n",
    "            joint = np.zeros((21, 4))\n",
    "        \n",
    "            for j, lm in enumerate(res.landmark):\n",
    "                joint[j] = [lm.x, lm.y, lm.z, lm.visibility]\n",
    "\n",
    "            v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3]\n",
    "            v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3]\n",
    "            v = v2 - v1 # [20, 3]\n",
    "\n",
    "            v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "            angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "                v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "\n",
    "            angle = np.degrees(angle)\n",
    "\n",
    "            data = angle\n",
    "\n",
    "            # data = np.array([angle], dtype=np.float32)\n",
    "\n",
    "            seq.append(data)\n",
    "\n",
    "            mp_drawing.draw_landmarks(img,\n",
    "                                      res,\n",
    "                                      mp_hands.HAND_CONNECTIONS\n",
    "                                    #   mp_hands.get_default_hand_landmarks_style(),\n",
    "                                    #   mp_hands.get_default_hand_connections_style()\n",
    "            )\n",
    "            \n",
    "            if len(seq) < seq_length:\n",
    "                continue\n",
    "\n",
    "            # print(seq[-seq_length:])\n",
    "\n",
    "            input_data = np.expand_dims(np.array(seq[-seq_length:], dtype=np.float32), axis=0)\n",
    "\n",
    "            # print(input_data)\n",
    "            y_pred = model.predict(input_data).squeeze()\n",
    "\n",
    "            i_pred = int(np.argmax(y_pred))\n",
    "            conf = y_pred[i_pred]\n",
    "\n",
    "            if conf < 0.9:\n",
    "                continue\n",
    "\n",
    "            action = actions[i_pred]\n",
    "            action_seq.append(action)\n",
    "\n",
    "            if len(action_seq) < 8:\n",
    "                continue\n",
    "\n",
    "            if len(action_seq) >= 8:\n",
    "                prev_action = action_seq[-8]\n",
    "                current_action = action_seq[-1]\n",
    "\n",
    "            this_action = '?'\n",
    "            if action_seq[-1] == action_seq[-2] == action_seq[-3]:\n",
    "                this_action = action\n",
    "                this_num = i_pred\n",
    "            \n",
    "            # 모자이크 처리\n",
    "            if i_pred == 7:\n",
    "                x1, y1 = tuple((joint.min(axis=0)[:2] * [img.shape[1], img.shape[0]] * 0.95).astype(int))\n",
    "                x2, y2 = tuple((joint.max(axis=0)[:2] * [img.shape[1], img.shape[0]] * 1.05).astype(int))\n",
    "\n",
    "                fy_img = img[y1:y2, x1:x2].copy()\n",
    "                fy_img = cv2.resize(fy_img, dsize=None, fx=0.05, fy=0.05, interpolation=cv2.INTER_NEAREST)\n",
    "                fy_img = cv2.resize(fy_img, dsize=(x2 - x1, y2 - y1), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                img[y1:y2, x1:x2] = fy_img\n",
    "\n",
    "            cv2.putText(img, f'{this_action.upper()}', org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=2, color=(0, 255, 0), thickness=7)\n",
    "\n",
    "            if i_pred == 5:\n",
    "                key_input_mode = 'ON'\n",
    "\n",
    "            if key_input_mode == 'ON':\n",
    "                # key 입력\n",
    "                if i_pred == 0:\n",
    "                    pyautogui.keyDown('w')\n",
    "                    action_done = True\n",
    "                \n",
    "                elif i_pred == 1:\n",
    "                    if action_done == True:\n",
    "                        pyautogui.keyUp('w')\n",
    "                        pyautogui.keyUp('s')\n",
    "                        action_done = False\n",
    "                    pyautogui.keyDown('s')\n",
    "                    action_done = True\n",
    "\n",
    "                elif i_pred == 2:\n",
    "                    if action_done == True:\n",
    "                        pyautogui.keyUp('w')\n",
    "                        pyautogui.keyUp('s')\n",
    "            \n",
    "            if i_pred == 6:\n",
    "                key_input_mode = 'OFF'\n",
    "\n",
    "            if key_input_mode == 'OFF':\n",
    "                if action_done == True:\n",
    "                    pyautogui.keyUp('w')\n",
    "                    pyautogui.keyUp('s')\n",
    "                    action_done = False\n",
    "                    \n",
    "                # key 입력\n",
    "                if i_pred == 0:\n",
    "                    pyautogui.press('w')\n",
    "                    action_done = False\n",
    "                \n",
    "                elif i_pred == 1:\n",
    "                    pyautogui.press('s')\n",
    "                    action_done = False\n",
    "\n",
    "                elif i_pred == 2:\n",
    "                    if action_done == True:\n",
    "                        pyautogui.keyUp('w')\n",
    "                        pyautogui.keyUp('s')\n",
    "                        action_done = False\n",
    "\n",
    "            if i_pred == 3:\n",
    "                if action_done == True:\n",
    "                    pyautogui.keyUp('w')\n",
    "                    pyautogui.keyUp('s')\n",
    "                    action_done = False\n",
    "\n",
    "                pyautogui.moveTo(760,540)\n",
    "                action_done = False\n",
    "\n",
    "            # thumb_end = res.landmark[4]\n",
    "            # fist_end = res.landmark[17]\n",
    "\n",
    "            # if thumb_end.y - fist_end.y > THRESHOLD:\n",
    "            #     # text = 'DOWN'\n",
    "            #     pyautogui.moveTo(760, 200)\n",
    "            #     action_done = False\n",
    "\n",
    "\n",
    "            elif i_pred == 4:\n",
    "                if action_done == True:\n",
    "                    pyautogui.keyUp('w')\n",
    "                    pyautogui.keyUp('s')\n",
    "                    action_done = False\n",
    "\n",
    "                pyautogui.moveTo(1160,540)\n",
    "                action_done = False\n",
    "\n",
    "        cv2.putText(img, f'Prev Action: {prev_action.upper()}', (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.putText(img, f'Current Action: {current_action.upper()}', (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Test', img)\n",
    "\n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "    if key == 27:\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02-1. KNN - 데이터셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_hands = 1\n",
    "count_click = 0\n",
    "\n",
    "# 제스처 클래스 정의\n",
    "gestures = {\n",
    "    0:'go', 1:'back', 2:'stop', 3:'left_spin', 4:'right_spin', 5:'speed_up',\n",
    "    6:'speed_down', 7:'bad_gesture'}\n",
    "action_label = 0\n",
    "action = str(gestures[action_label])\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=max_num_hands,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "\n",
    "file = np.genfromtxt('../data/gesture_train_tovstack.csv', delimiter=',')\n",
    "print(\"csv파일 로드됨\", file.shape)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 클릭 이벤트\n",
    "# 화면을 클릭했을 때 각도 값을 csv파일에 추가\n",
    "def click(event, x, y, flags, param):\n",
    "    global data, file, count_click\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        count_click += 1\n",
    "        file = np.vstack((file, data))  # numpy의 vstack 사용\n",
    "        print(file.shape)\n",
    "\n",
    "cv2.namedWindow('Dataset')\n",
    "cv2.setMouseCallback('Dataset', click)\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    ret, img = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"카메라 연결 실패\")\n",
    "        # break\n",
    "        continue\n",
    "\n",
    "    img = cv2.flip(img, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    cv2.putText(img, action, org=(10, 30), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(0, 0, 255), thickness=3)\n",
    "    cv2.putText(img, str(count_click), org=(10, 80), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "    \n",
    "    # 손 인식 여부 확인\n",
    "    if result.multi_hand_landmarks is not None:\n",
    "\n",
    "        for res in result.multi_hand_landmarks:\n",
    "\n",
    "            joint = np.zeros((21, 3))\n",
    "\n",
    "            for j, lm in enumerate(res.landmark):\n",
    "                joint[j] = [lm.x, lm.y, lm.z]\n",
    "\n",
    "            v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19],:]\n",
    "            v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],:]\n",
    "            v = v2 - v1 # [20,3]\n",
    "\n",
    "            v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "            angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "                v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "\n",
    "            angle = np.degrees(angle)\n",
    "\n",
    "            data = np.array([angle], dtype=np.float32)\n",
    "            data = np.append(data, action_label)\n",
    "            \n",
    "            # 각도값 출력 확인\n",
    "            # print(data)\n",
    "\n",
    "            mp_drawing.draw_landmarks(img,\n",
    "                                      res,\n",
    "                                      mp_hands.HAND_CONNECTIONS\n",
    "                                    #   mp_hands.get_default_hand_landmarks_style(),\n",
    "                                    #   mp_hands.get_default_hand_connections_style()\n",
    "            )\n",
    "\n",
    "    cv2.imshow('Dataset', img)\n",
    "\n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "    if key == ord('a'):\n",
    "        created_time = str(time.strftime('%X', time.localtime(time.time())))\n",
    "        os.makedirs('../data/dataset', exist_ok=True)\n",
    "        np.savetxt(os.path.join('../data/dataset', f'{action}_{created_time}.csv'), file[1:], delimiter=',')\n",
    "        \n",
    "        if action_label == 7:\n",
    "            action_label = 0\n",
    "        else:\n",
    "            action_label += 1\n",
    "        action = str(gestures[action_label])\n",
    "        count_click = 0\n",
    "\n",
    "        file = np.genfromtxt('../data/gesture_train_tovstack.csv', delimiter=',')\n",
    "        print(\"csv파일 로드됨\", file.shape)\n",
    "\n",
    "    if key == 27:\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "        break\n",
    "\n",
    "\n",
    "created_time = str(time.strftime('%X', time.localtime(time.time())))\n",
    "# exist_ok를 True로 설정하지 않았을 땐, 해당 디렉토리가 존재하는 경우 exception 에러 발생\n",
    "os.makedirs('../data/dataset', exist_ok=True)\n",
    "# 한 동작만 저장\n",
    "action = str(gestures[action_label])\n",
    "np.savetxt(os.path.join('../data/dataset', f'{action}_{created_time}.csv'), file[1:], delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02-2. KNN - 성능 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제스처당 10개 dataset\n",
    "df_file_10_go = pd.read_csv('../data/dataset_10_231211/go_04:54:34 PM.csv', header=None)\n",
    "df_file_10_back = pd.read_csv('../data/dataset_10_231211/back_04:55:04 PM.csv', header=None)\n",
    "df_file_10_stop = pd.read_csv('../data/dataset_10_231211/stop_04:55:31 PM.csv', header=None)\n",
    "df_file_10_left = pd.read_csv('../data/dataset_10_231211/left_spin_04:55:49 PM.csv', header=None)\n",
    "df_file_10_right = pd.read_csv('../data/dataset_10_231211/right_spin_04:56:08 PM.csv', header=None)\n",
    "df_file_10_up = pd.read_csv('../data/dataset_10_231211/speed_up_04:57:54 PM.csv', header=None)\n",
    "df_file_10_down = pd.read_csv('../data/dataset_10_231211/speed_down_04:58:18 PM.csv', header=None)\n",
    "df_file_10_bad = pd.read_csv('../data/dataset_10_231211/bad_gesture_05:00:03 PM.csv', header=None)\n",
    "\n",
    "df_file_10 = pd.concat([df_file_10_go, df_file_10_back, df_file_10_stop, df_file_10_left, df_file_10_right, df_file_10_up, df_file_10_down, df_file_10_bad], ignore_index=True)\n",
    "df_file_10.to_csv(\"../data/dataset_10_231211/total_10.csv\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제스처당 100개 dataset\n",
    "df_file_100_go = pd.read_csv('../data/dataset_100_231211_2/go_05:04:05 PM.csv', header=None)\n",
    "df_file_100_back = pd.read_csv('../data/dataset_100_231211_2/back_05:06:55 PM.csv', header=None)\n",
    "df_file_100_stop = pd.read_csv('../data/dataset_100_231211_2/stop_05:09:41 PM.csv', header=None)\n",
    "df_file_100_left = pd.read_csv('../data/dataset_100_231211_2/left_spin_05:12:50 PM.csv', header=None)\n",
    "df_file_100_right = pd.read_csv('../data/dataset_100_231211_2/right_spin_05:15:07 PM.csv', header=None)\n",
    "df_file_100_up = pd.read_csv('../data/dataset_100_231211_2/speed_up_05:18:02 PM.csv', header=None)\n",
    "df_file_100_down = pd.read_csv('../data/dataset_100_231211_2/speed_down_05:20:26 PM.csv', header=None)\n",
    "df_file_100_bad = pd.read_csv('../data/dataset_100_231211_2/bad_gesture_05:23:23 PM.csv', header=None)\n",
    "\n",
    "df_file_100 = pd.concat([df_file_100_go, df_file_100_back, df_file_100_stop, df_file_100_left, df_file_100_right, df_file_100_up, df_file_100_down, df_file_100_bad], ignore_index=True)\n",
    "df_file_100.to_csv(\"../data/dataset_100_231211_2/total_100.csv\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = np.genfromtxt('../data/dataset_10_231211/total_10.csv', delimiter=',')\n",
    "file = np.genfromtxt('../data/dataset_100_231211/total_100.csv', delimiter=',')\n",
    "# file = np.genfromtxt('../data/dataset_2_1/bad_gesture_03:15:29 AM.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x데이터와 y데이터 나누기\n",
    "# train셋, test셋 split\n",
    "# file = np.genfromtxt('../data/dataset_2_1/bad_gesture_03:15:29 AM.csv', delimiter=',')\n",
    "angle = file[:,:-1].astype(np.float32)\n",
    "label = file[:, -1].astype(np.float32)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(angle,\n",
    "                                                                    label,\n",
    "                                                                    test_size = 0.2,\n",
    "                                                                    random_state = 2023,\n",
    "                                                                    stratify=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN 모델 생성 후 학습\n",
    "knn = cv2.ml.KNearest_create()\n",
    "knn.train(X_train, cv2.ml.ROW_SAMPLE, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산\n",
    "ret, result, neighbours, dist = knn.findNearest(np.array(X_test, dtype=np.float32), k=3)\n",
    "accuracy = accuracy_score(y_test, result.flatten())\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# y_pred = knn.predict(X_test)\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 인식 결과를 단순히 출력하도록\n",
    "# max_num_hands = 1\n",
    "\n",
    "# gestures = {\n",
    "#     0:'go', 1:'back', 2:'stop', 3:'left_spin', 4:'right_spin', 5:'speed_up',\n",
    "#     6:'speed_down', 7:'bad_gesture'}\n",
    "\n",
    "# mp_hands = mp.solutions.hands\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# hands = mp_hands.Hands(\n",
    "#     max_num_hands=max_num_hands,\n",
    "#     min_detection_confidence=0.5,\n",
    "#     min_tracking_confidence=0.5)\n",
    "\n",
    "# file = np.genfromtxt('../data/dataset_10_231211/total_10.csv', delimiter=',')\n",
    "# angle = file[:,:-1].astype(np.float32)\n",
    "# label = file[:, -1].astype(np.float32)\n",
    "# knn = cv2.ml.KNearest_create()\n",
    "# knn.train(angle, cv2.ml.ROW_SAMPLE, label)\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# while cap.isOpened():\n",
    "#     ret, img = cap.read()\n",
    "\n",
    "#     if not ret:\n",
    "#         print(\"카메라 연결 실패\")\n",
    "#         break\n",
    "#         # continue\n",
    "\n",
    "#     img = cv2.flip(img, 1)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     result = hands.process(img)\n",
    "\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#     if result.multi_hand_landmarks is not None:\n",
    "\n",
    "#         for res in result.multi_hand_landmarks:\n",
    "\n",
    "#             joint = np.zeros((21, 3))\n",
    "\n",
    "#             for j, lm in enumerate(res.landmark):\n",
    "#                 joint[j] = [lm.x, lm.y, lm.z]\n",
    "\n",
    "#             v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19],:]\n",
    "#             v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],:]\n",
    "#             v = v2 - v1 # [20,3]\n",
    "\n",
    "#             v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "#             angle = np.arccos(np.einsum('nt,nt->n',\n",
    "#                 v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "#                 v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "\n",
    "#             angle = np.degrees(angle)\n",
    "\n",
    "#             data = np.array([angle], dtype=np.float32)\n",
    "#             ret, results, neighbours, dist = knn.findNearest(data, 5)\n",
    "#             idx = int(results[0][0])\n",
    "\n",
    "#             if idx in gestures.keys():\n",
    "#                 cv2.putText(img, text=gestures[idx].upper(), org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "\n",
    "#             mp_drawing.draw_landmarks(img,\n",
    "#                                       res,\n",
    "#                                       mp_hands.HAND_CONNECTIONS\n",
    "#                                     #   mp_hands.get_default_hand_landmarks_style(),\n",
    "#                                     #   mp_hands.get_default_hand_connections_style()\n",
    "#             )\n",
    "    \n",
    "#     cv2.imshow('Test', img)\n",
    "\n",
    "#     key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "#     if key == 27:\n",
    "#         cv2.destroyAllWindows()\n",
    "#         cap.release()\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인식 결과를 단순히 출력하도록\n",
    "max_num_hands = 1\n",
    "\n",
    "gestures = {\n",
    "    0:'go', 1:'back', 2:'stop', 3:'left_spin', 4:'right_spin', 5:'speed_up',\n",
    "    6:'speed_down', 7:'bad_gesture'}\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=max_num_hands,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "\n",
    "file = np.genfromtxt('../data/dataset_100_231211/total_100.csv', delimiter=',')\n",
    "angle = file[:,:-1].astype(np.float32)\n",
    "label = file[:, -1].astype(np.float32)\n",
    "knn = cv2.ml.KNearest_create()\n",
    "knn.train(angle, cv2.ml.ROW_SAMPLE, label)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "fps = 0\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# prev_action = '?'\n",
    "# current_action = '?'\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"카메라 연결 실패\")\n",
    "        # break\n",
    "        continue\n",
    "\n",
    "    frame_count += 1\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time > 1.0:  # 1초마다 FPS 갱신\n",
    "        fps = frame_count / elapsed_time\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    img = cv2.flip(img, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result = hands.process(img)\n",
    "\n",
    "    cv2.putText(img, 'KNN', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if result.multi_hand_landmarks is not None:\n",
    "\n",
    "        for res in result.multi_hand_landmarks:\n",
    "\n",
    "            joint = np.zeros((21, 3))\n",
    "\n",
    "            for j, lm in enumerate(res.landmark):\n",
    "                joint[j] = [lm.x, lm.y, lm.z]\n",
    "\n",
    "            v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19],:]\n",
    "            v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],:]\n",
    "            v = v2 - v1 # [20,3]\n",
    "\n",
    "            v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "            angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "                v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "\n",
    "            angle = np.degrees(angle)\n",
    "\n",
    "            data = np.array([angle], dtype=np.float32)\n",
    "            ret, results, neighbours, dist = knn.findNearest(data, 5)\n",
    "            idx = int(results[0][0])\n",
    "\n",
    "            if idx in gestures.keys():\n",
    "                cv2.putText(img, text=gestures[idx].upper(), org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=2, color=(0, 255, 0), thickness=7)\n",
    "\n",
    "            mp_drawing.draw_landmarks(img,\n",
    "                                      res,\n",
    "                                      mp_hands.HAND_CONNECTIONS\n",
    "                                    #   mp_hands.get_default_hand_landmarks_style(),\n",
    "                                    #   mp_hands.get_default_hand_connections_style()\n",
    "            )\n",
    "\n",
    "            # if idx == 7:\n",
    "            #     x1, y1 = tuple((joint.min(axis=0)[:2] * [img.shape[1], img.shape[0]] * 0.95).astype(int))\n",
    "            #     x2, y2 = tuple((joint.max(axis=0)[:2] * [img.shape[1], img.shape[0]] * 1.05).astype(int))\n",
    "\n",
    "            #     fy_img = img[y1:y2, x1:x2].copy()\n",
    "            #     fy_img = cv2.resize(fy_img, dsize=None, fx=0.05, fy=0.05, interpolation=cv2.INTER_NEAREST)\n",
    "            #     fy_img = cv2.resize(fy_img, dsize=(x2 - x1, y2 - y1), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            #     img[y1:y2, x1:x2] = fy_img\n",
    "            \n",
    "        cv2.putText(img, f'Action: {idx}. {gestures[idx].upper()}', (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        # cv2.putText(img, f'Current Action: {current_action.upper()}', (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow('Test', img)\n",
    "\n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "    if key == 27:\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03-1. RNN - 모델 생성 후 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_file = pd.read_csv('../data/dataset_10_231211/total_10.csv', delimiter=',', header=None)\n",
    "df_file = pd.read_csv('../data/dataset_100_231211/total_100.csv', delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x데이터와 y데이터 나누기\n",
    "# train셋, test셋 split\n",
    "# df_file = pd.read_csv('/home/ckdal/dev_ws/project/Dl_Project/data/dataset_knn_1211/bad_gesture_01:03:08 PM.csv', header=None)\n",
    "df_angle = df_file.iloc[:, 0:15]\n",
    "df_label = df_file.iloc[:, -1]\n",
    "\n",
    "# 훈련 데이터는 전체 데이터를 대표할 수 있도록 라벨이 골고루 포함되어야 함\n",
    "# stratify: 원래 데이터의 분포와 유사하게 데이터를 추출해주는 파라미터\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(df_angle,\n",
    "                                                                    df_label,\n",
    "                                                                    test_size = 0.2,\n",
    "                                                                    random_state = 2023,\n",
    "                                                                    stratify=df_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "encoder = OneHotEncoder(sparse_output = False)\n",
    "\n",
    "y_train_np = y_train.to_numpy()\n",
    "# 1D 배열을 열이 1인 2D 배열로 변환\n",
    "y_train_2d = y_train_np.reshape(-1, 1)\n",
    "y_train_encoded = encoder.fit_transform(y_train_2d)\n",
    "\n",
    "y_test_np = y_test.to_numpy()\n",
    "y_test_2d = y_test_np.reshape(-1, 1)\n",
    "y_test_encoded = encoder.fit_transform(y_test_2d)\n",
    "\n",
    "# y_train_encoded\n",
    "y_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded.shape\n",
    "# y_test_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 모델 생성\n",
    "# 이진 분류\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Dense(input_dim=15, units=64, activation=None, kernel_initializer=initializers.he_uniform()))\n",
    "\n",
    "# model.add(layers.Activation('elu'))\n",
    "\n",
    "# model.add(layers.Dense(units=32, activation=None, kernel_initializer=initializers.he_uniform())) \n",
    "# model.add(layers.Activation('elu')) \n",
    "\n",
    "# model.add(layers.Dense(units=32, activation=None, kernel_initializer=initializers.he_uniform())) \n",
    "# model.add(layers.Activation('elu'))\n",
    "\n",
    "# model.add(layers.Dropout(rate=0.5))\n",
    "\n",
    "# # 출력 레이어\n",
    "# model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# model.compile(optimizer=optimizers.Adam(),\n",
    "#               loss=losses.binary_crossentropy,\n",
    "#               metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "# 다중 클래스 분류\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(input_dim=15, units=64, activation=None, kernel_initializer=initializers.he_uniform()))\n",
    "\n",
    "model.add(layers.Activation('elu'))\n",
    "\n",
    "model.add(layers.Dense(units=32, activation=None, kernel_initializer=initializers.he_uniform())) \n",
    "model.add(layers.Activation('elu')) \n",
    "\n",
    "model.add(layers.Dense(units=32, activation=None, kernel_initializer=initializers.he_uniform())) \n",
    "model.add(layers.Activation('elu'))\n",
    "\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "\n",
    "# 출력 레이어\n",
    "model.add(layers.Dense(units=8, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(),\n",
    "              loss=losses.categorical_crossentropy,\n",
    "              metrics=[metrics.categorical_accuracy]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(layers.LSTM(64, return_sequences=True,\n",
    "#                 input_shape=(70, 84))) \n",
    "\n",
    "# model.add(layers.LSTM(32, return_sequences=True))\n",
    "\n",
    "# model.add(layers.LSTM(32))\n",
    "\n",
    "# model.add(layers.Dense(8, activation='softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#                 optimizer='adam',\n",
    "#                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train_encoded, batch_size = 200, epochs = 500, validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도\n",
    "result = model.evaluate(X_test, y_test_encoded)\n",
    "\n",
    "print('loss (cross-entropy) :', result[0])\n",
    "print('test accuracy :', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 모델\n",
    "# 이진 분류\n",
    "# fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "# acc_ax = loss_ax.twinx()\n",
    "\n",
    "# loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "# loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "# loss_ax.set_xlabel('epoch')\n",
    "# loss_ax.set_ylabel('loss')\n",
    "# loss_ax.legend(loc='upper left')\n",
    "\n",
    "# acc = history.history['binary_accuracy']\n",
    "# val_acc = history.history['val_binary_accuracy']\n",
    "\n",
    "# acc_ax.plot(acc, 'b', label='train acc')\n",
    "# acc_ax.plot(val_acc, 'g', label='val acc')\n",
    "# acc_ax.set_ylabel('accuracy')\n",
    "# acc_ax.legend(loc='upper left')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 분류\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "acc_ax.plot(acc, 'b', label='train acc')\n",
    "acc_ax.plot(val_acc, 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 모델\n",
    "# 이진 분류\n",
    "fig, axes = plt.subplots(1, 2, figsize = (12, 4))\n",
    "\n",
    "sns.lineplot(x = range(len(history.history[\"loss\"])),\n",
    "             y = history.history[\"loss\"], ax = axes[0],\n",
    "             label = 'Training Loss')\n",
    "\n",
    "sns.lineplot(x = range(len(history.history[\"loss\"])),\n",
    "             y = history.history[\"val_loss\"], ax = axes[0],\n",
    "             label = 'Validation Loss')\n",
    "\n",
    "\n",
    "sns.lineplot(x = range(len(history.history[\"categorical_accuracy\"])),\n",
    "             y = history.history[\"categorical_accuracy\"], ax = axes[1],\n",
    "             label = 'Training Accuracy')\n",
    "\n",
    "sns.lineplot(x = range(len(history.history[\"categorical_accuracy\"])),\n",
    "             y = history.history[\"val_categorical_accuracy\"], ax = axes[1],\n",
    "             label = 'Validation Accuracy')\n",
    "axes[0].set_title(\"Loss\"); axes[1].set_title(\"Accuracy\")\n",
    "\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save('../model/model_100_231211_2_epoch500.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03-2. RNN - 성능 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_angle_test = pd.DataFrame(X_test.iloc[13,:])\n",
    "df_angle_test = df_angle_test.transpose()\n",
    "df_angle_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = keras.models.load_model(\"../model/model_epoch500.h5\")\n",
    "# angle_test = np.array([[2.356400489807128906e+01,6.761012554168701172e+00,6.285949230194091797e+00,8.304659843444824219e+00,6.339200019836425781e+00,4.894902706146240234e+00,4.375961303710937500e+00,6.588742256164550781e+00,4.068197727203369141e+00,7.041219234466552734e+00,5.034504413604736328e+00,4.858801364898681641e+00,1.002417564392089844e+01,7.321394920349121094e+00,4.523040771484375000e+00]])\n",
    "# angle_test = df_\n",
    "print(encoder.inverse_transform(model_test.predict(df_angle_test))[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카메라 인덱스 확인\n",
    "# sudo apt-get install v4l-utils -y\n",
    "# v4l2-ctl --list-devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인식 결과를 단순히 출력하도록\n",
    "max_num_hands = 1\n",
    "actions = ['go', 'back', 'stop', 'left_spin', 'right_spin', 'speed_up', 'speed_down', 'bad_gesture']\n",
    "gestures = {\n",
    "    0:'go', 1:'back', 2:'stop', 3:'left_spin', 4:'right_spin', 5:'speed_up',\n",
    "    6:'speed_down', 7:'bad_gesture'}\n",
    "\n",
    "# encoder = OneHotEncoder(sparse_output = False)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=max_num_hands,                            \n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "\n",
    "model = keras.models.load_model(\"../model/RNN_model_dataset100_epoch500_231211.h5\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "seq = []\n",
    "action_seq = [] \n",
    "\n",
    "fps = 0\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    ret, img = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"카메라 연결 실패\")\n",
    "        # break\n",
    "        continue\n",
    "\n",
    "    frame_count += 1\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time > 1.0:  # 1초마다 FPS 갱신\n",
    "        fps = frame_count / elapsed_time\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    img = cv2.flip(img, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result = hands.process(img)\n",
    "\n",
    "    cv2.putText(img, 'RNN', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if result.multi_hand_landmarks is not None:\n",
    "\n",
    "        for res in result.multi_hand_landmarks:\n",
    "\n",
    "            joint = np.zeros((21, 3))\n",
    "\n",
    "            for j, lm in enumerate(res.landmark):\n",
    "                joint[j] = [lm.x, lm.y, lm.z]\n",
    "\n",
    "            v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19],:]\n",
    "            v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],:]\n",
    "            v = v2 - v1 # [20,3]\n",
    "\n",
    "            v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "            angles = []\n",
    "\n",
    "            angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "                v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "            \n",
    "            angle = np.degrees(angle)\n",
    "\n",
    "            data = np.array([angle], dtype=np.float32)\n",
    "            \n",
    "            idx = encoder.inverse_transform(model.predict([data]))[0,0]\n",
    "\n",
    "            if idx in gestures.keys():\n",
    "                cv2.putText(img, text=gestures[idx].upper(), org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=2, color=(0, 255, 0), thickness=7)\n",
    "\n",
    "            mp_drawing.draw_landmarks(img,\n",
    "                                      res,\n",
    "                                      mp_hands.HAND_CONNECTIONS\n",
    "                                    #   mp_hands.get_default_hand_landmarks_style(),\n",
    "                                    #   mp_hands.get_default_hand_connections_style()\n",
    "            )\n",
    "        \n",
    "        cv2.putText(img, f'Action: {int(idx)}. {gestures[idx].upper()}', (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.imshow('Test', img)\n",
    "\n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "    if key == 27:\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
